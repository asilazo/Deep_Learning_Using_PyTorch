{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0fec8b102f7b88d2f49c12802992373495edaa2417752b343d572adbca691d53d",
   "display_name": "Python 3.8.10 64-bit ('tnsflow2_gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Building Neural Network Models\n",
    "\n",
    "In this recipe, we will start by understanding some important functions of TorchVision \n",
    "that enable it to deal with image data and process it. We will then define a basic \n",
    "architecture for a neural network by defining a class, and look at the modules and methods \n",
    "available for this. In this recipe, we will be focusing on a fully connected neural network \n",
    "class. Its attributes are the various layers whose purpose is to classify various types of \n",
    "clothes.\n",
    "\n",
    "## Description of the dataset\n",
    "\n",
    "We will be using the Fashion–MNIST dataset. This is a dataset of Zalando's article images, \n",
    "consisting of a training set of 60,000 examples and a test set of 10,000 examples. We will \n",
    "take an individual grayscale image 28 x 28 in size and convert it into a vector of 784.\n",
    "\n",
    "\n",
    "## Note\n",
    "\n",
    "This recipe should be used in conjuction with the recipe named \"02_Loading Data Using PyTorch\",\n",
    "to pull the MNIST data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for the preprocessing of our image data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert the input to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,)), # normalize the data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batch_size to divide our dataset into chunks to be fed into the model\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# path to data\n",
    "path = \"./Data\"\n",
    "\n",
    "train_data = datasets.FashionMNIST(path, train=True, download=False,transform=transform) # setting download=False since I already have the data downloaded\n",
    "\n",
    "# training data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# test data\n",
    "\n",
    "test_data = datasets.FashionMNIST(path, train=False, transform=transform)\n",
    "\n",
    "# test data loader\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "source": [
    "Our main task here is to define the neural network class, which will be a subclass of nn.Module.\n",
    "\n",
    "## *Important Notes*\n",
    "\n",
    "We could define the model class with any name, but what is important is that it is a subclass of \n",
    "nn.Module and has super().__init__(), which provides the model with a lot of useful methods and \n",
    "attributes and retains knowledge of the architecture.\n",
    "\n",
    "We use nn.Linear() to define fully connected layers by passing in the input and output dimensions.\n",
    "We use a softmax layer for the last layer output because there are 10 output classes. We use ReLU \n",
    "activation in the layers before the output layer to learn nonlinearity in the data. The hidden1 \n",
    "layer takes 784 inputs units and gives out 256 output units. The hidden2 phrase outputs 128 units \n",
    "and the output layer has 10 output units representing 10 output classes. The softmax layer converts \n",
    "the activations into probabilities so that it adds to 1 along dimension 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(784, 256)  # first hidden layer\n",
    "        self.hidden2 = nn.Linear(256, 128)  # second hidden layer\n",
    "        self.output = nn.Linear(128, 10)    # output layer\n",
    "        self.activation = nn.ReLU()         # activation function for the inner layers\n",
    "        self.softmax = nn.Softmax(dim=1)    # softmax activation layer\n",
    "        \n",
    "\n",
    "    \n",
    "    def foward(self, x):\n",
    "        x = self.hidden1(x) # move the input to the first hidden layer, with 256 nodes\n",
    "        x = self.activation(x)  # pass the outputs from the first hidden layer through \n",
    "                                #the activation function, which in our case is ReLU\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)      # pass the last output layer, with 10 output classes\n",
    "        \n",
    "        output = self.softmax(x)    # push the output using the softmax function\n",
    "\n",
    "        return output               # return the output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network object\n",
    "model = FashionNetwork()\n",
    "\n",
    "# a quick look at our model\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "## Summary Notes\n",
    "\n",
    "A network defined with nn.Module needs to have a forward() method defined. It takes the \n",
    "input tensor and passes it through the network components defined in the __init__() method \n",
    "in the network class, in the sequence of operations defined in the forward method.\n",
    "\n",
    "The forward method is called automatically when input is passed referring to the name of \n",
    "the model object. The nn.Module automatically creates the weight and bias tensors that we'll \n",
    "use in the forward method. The linear unit by itself defines a linear function, such as \n",
    "*xW* + *B*; to have nonlinear capabilities, we need to insert nonlinear activation functions, \n",
    "and here we use one of the most popular activation functions, ReLU, although you could use other \n",
    "available activation functions in PyTorch.\n",
    "\n",
    "The reason we squish the final layer output through softmax is because we want to have 1 output \n",
    "class with a higher probability than all the other classes, and the sum of the output probabilities \n",
    "should equal 1. The softmax function has a parameter dim=1 that ensures that softmax is taken across \n",
    "the columns of the output.\n",
    "\n",
    "We can define the network architecture without defining a network class using the *nn.Sequential* module, \n",
    "and it is important to ensure that the sequence of operation in the *forward* method is ordered properly, \n",
    "although the sequence doesn't matter in \\__init__. You can use *nn.Tanh* for tanh activation. You can \n",
    "access the weight and bias tensors from the model object with *model.hidden.weight* and *model.hidden.bias*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Defining the loss function\n",
    "\n",
    "A machine learning model, when being trained, may have some deviation between the predicted output and the \n",
    "actual output, and this difference is called the *error* of the model. The function that lets us calculate \n",
    "this *error* is called the *loss function*, or *error function*. This function provides a metric to evaluate \n",
    "all possible solutions and choose the most optimized model. The loss function has to be able to reduce all \n",
    "attributes of the model down to a single number so that an improvement in that *loss function value* is \n",
    "representative of a better model. \n",
    "\n",
    "In this recipe, we will define a *loss function* for our fashion dataset using the loss function available in \n",
    "PyTorch.\n",
    "\n",
    "** First, we will modify our existing network architecture to the output log of softmax instead of softmax, \n",
    "starting with the \\__init__ method in the network constructor.\n",
    "\n",
    "** Next, we will make the same change in the forward method of the neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(784, 256)  # first hidden layer\n",
    "        self.hidden2 = nn.Linear(256, 128)  # second hidden layer\n",
    "        self.output = nn.Linear(128, 10)    # output layer\n",
    "        self.activation = nn.ReLU()         # activation function for the inner layers\n",
    "        self.log_softmax = nn.LogSoftmax()    # softmax activation layer\n",
    "        \n",
    "\n",
    "    \n",
    "    def foward(self, x):\n",
    "        x = self.hidden1(x) # move the input to the first hidden layer, with 256 nodes\n",
    "        x = self.activation(x)  # pass the outputs from the first hidden layer through \n",
    "                                #the activation function, which in our case is ReLU\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)      # pass the last output layer, with 10 output classes\n",
    "        \n",
    "        output = self.log_softmax(x)    # push the output using the softmax function\n",
    "\n",
    "        return output               # return the output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionNetwork()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our loss function; we will use negative log likelihood loss for this\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "source": [
    "## Section Notes\n",
    "\n",
    "We replaced softmax with log softmax so that we could then use the log of probabilities \n",
    "over probabilities, which has nice theoretic interpretations. There are various reasons \n",
    "for doing this, including improved numerical performance and gradient optimization. \n",
    "These advantages can be extremely important when training a model that can be computationally \n",
    "challenging and expensive. Furthermore, it has a high penalizing effect when it is not \n",
    "predicting the correct class. We therefore use negative log likelihood when dealing with \n",
    "*log softmax*, as *softmax* is not compatible. It is useful in classification between *n* \n",
    "number of classes. The log would ensure that we are not dealing with very small values between \n",
    "0 and 1, and negative values would ensure that a *logarithm* of probability that is less than 1 \n",
    "is nonzero. Our goal would be to reduce this negative log loss error function. In PyTorch, the \n",
    "loss function is called a *criterion*, and so we named our loss function criterion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Implementing optimizers                                                        \n",
    "In the previous recipe, Defining the loss function, we spoke of errors and error functions, and \n",
    "learned that, for us to get a good model, we need to minimize the errors that are calculated. \n",
    "*Backpropagation* is a method by which the neural networks learn from errors; the errors are used \n",
    "to modify weights in such a way that the errors are minimized. *Optimization* functions are responsible \n",
    "for modifying weights to reduce the error. *Optimization* functions calculate the *partial derivative* \n",
    "of errors with respect to *weights*. The *derivative* shows the direction of a slope, and so we need to \n",
    "reverse the direction of the *gradient*. The *optimizer function* combines the *model parameters* and \n",
    "*loss function* to iteratively modify the model parameters to reduce the model error. *Optimizers* can \n",
    "be thought of as fiddling with the model weights to get the best possible model based on the difference \n",
    "in prediction from the model and the actual output, and the *loss function* acts as a guide by indicating \n",
    "when the *optimizer* is going right or wrong. The *learning rate* is a *hyperparameter* of the *optimizer*, \n",
    "which controls the amount by which the *weights* are updated. The *learning rate* ensures that the *weights* \n",
    "are not updated by a huge amount so that the algorithm fails to converge at all and the error gets bigger \n",
    "and bigger; however at the same time, the updating of the weight should not be so low that it takes forever \n",
    "to reach the *minimum* of the *cost function/error function*.\n",
    "\n",
    "In PyTorch, the optim module provides a number of *optimizers*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an optimizer object. We will use the Adam optimizer and pass model parameters\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# To check for the defaults of the optimizer, you can do the following\n",
    "optimizer.defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also add the learning rate as an additional parameter\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotImplementedError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-72ba43d8d923>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# get the prediction from the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# calculate the loss/error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tnsflow2_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tnsflow2_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \"\"\"\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's start training our model, starting with the number of epochs\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# start the training loop\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # initilize the loss\n",
    "    running_loss = 0\n",
    "\n",
    "    # iterate through each image in training the image loader, which we defined in an earlier\n",
    "    for image, label in train_loader.dataset:\n",
    "        # reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # reshape the image\n",
    "        image = image.view(image.shape[0], -1)\n",
    "\n",
    "        # get the prediction from the model\n",
    "        pred = model(image)\n",
    "\n",
    "        # calculate the loss/error\n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        # call the .backward() method on the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # call the .step() method on the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # append to the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Finally, print the loss after each epoch\n",
    "    else:\n",
    "        print(f'Training loss: {running_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}